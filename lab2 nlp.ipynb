{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ef3aa909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import subprocess\n",
    "import sys\n",
    "import nltk\n",
    "from nltk import Nonterminal, nonterminals, Production, CFG\n",
    "\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    \n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "\n",
    "    Doc\n",
    ")\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "names_extractor = NamesExtractor(morph_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6c789b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    doc = Doc(sentence)\n",
    "    doc.segment(segmenter)\n",
    "    l = []\n",
    "    for token in doc.tokens:\n",
    "        l += [token.text]\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "31124ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parts_of_speech(sentence):\n",
    "    doc = Doc(sentence)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    \n",
    "    parts_of_speech = {}\n",
    "    l = []\n",
    "    for token in doc.tokens:\n",
    "        parts_of_speech[token.pos] = []\n",
    "    for key in parts_of_speech:\n",
    "        for token in doc.tokens:\n",
    "            if key == token.pos:\n",
    "                l += [token.text]\n",
    "                parts_of_speech[key] = l\n",
    "        l = []\n",
    "    return parts_of_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "639188d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tree(sentence: str) -> CFG:\n",
    "    grammar = \"\"\"\n",
    "    S -> NP VP | NP VP C VP\n",
    "    VP -> V NP | VP NP | Adv VP | V NP PrepP | VP NP PrepP | V V\n",
    "    NP -> N | Pronoun | AP NP\n",
    "    PrepP -> Prep NP\n",
    "    AP -> Adj | Adv Adj\n",
    "    \"\"\"\n",
    "#     grammar = \"\"\"\n",
    "#     S -> NP VP\n",
    "#     NP -> Adj N | N | Pronoun\n",
    "#     VP -> V NP | VP NP\n",
    "#     \"\"\"\n",
    "    parts_of_speech = get_parts_of_speech(sentence)\n",
    "    \n",
    "    if 'VERB' in parts_of_speech:\n",
    "        grammar += f\"\"\"\n",
    "        V -> {' | '.join(f\"'{w}'\" for w in parts_of_speech[\"VERB\"])}\n",
    "        \"\"\"\n",
    "    if 'NOUN' in parts_of_speech:\n",
    "        grammar += f\"\"\"\n",
    "        N -> {' | '.join(f\"'{w}'\" for w in parts_of_speech[\"NOUN\"])}\n",
    "        \"\"\" \n",
    "    if 'PRON' in parts_of_speech:\n",
    "        grammar += f\"\"\"\n",
    "        Pronoun -> {' | '.join(f\"'{w}'\" for w in parts_of_speech[\"PRON\"])}\n",
    "        \"\"\"\n",
    "    if 'ADP' in parts_of_speech:\n",
    "        grammar += f\"\"\"\n",
    "        Prep -> {' | '.join(f\"'{w}'\" for w in parts_of_speech[\"ADP\"])}\n",
    "        \"\"\"\n",
    "    if 'ADV' in parts_of_speech:\n",
    "        grammar += f\"\"\"\n",
    "        Adv -> {' | '.join(f\"'{w}'\" for w in parts_of_speech[\"ADV\"])}\n",
    "        \"\"\"\n",
    "    if 'ADJ' in parts_of_speech:\n",
    "        grammar += f\"\"\"\n",
    "        Adj -> {' | '.join(f\"'{w}'\" for w in parts_of_speech[\"ADJ\"])}\n",
    "        \"\"\"\n",
    "    if 'SCONJ' in parts_of_speech:\n",
    "        grammar += f\"\"\"\n",
    "        C -> {' | '.join(f\"'{w}'\" for w in parts_of_speech[\"SCONJ\"])}\n",
    "        \"\"\"\n",
    "#     grammar += f\"\"\"\n",
    "#     V -> {' | '.join(f\"'{w}'\" for w in parts_of_speech[\"VERB\"])}\n",
    "#     N -> {' | '.join(f\"'{w}'\" for w in parts_of_speech[\"NOUN\"])}\n",
    "#     Pronoun -> {' | '.join(f\"'{w}'\" for w in parts_of_speech[\"PRON\"])}\n",
    "#     Prep -> {' | '.join(f\"'{w}'\" for w in parts_of_speech[\"ADP\"])}\n",
    "#     Adv -> {' | '.join(f\"'{w}'\" for w in parts_of_speech[\"ADV\"])}\n",
    "#     Adj -> {' | '.join(f\"'{w}'\" for w in parts_of_speech[\"ADJ\"])}\n",
    "#     C -> {' | '.join(f\"'{w}'\" for w in parts_of_speech[\"SCONJ\"])}\n",
    "#     \"\"\"\n",
    "    return CFG.fromstring(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2648714a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S -> NP VP, S -> NP VP C VP, VP -> V NP, VP -> VP NP, VP -> Adv VP, VP -> V NP PrepP, VP -> VP NP PrepP, VP -> V V, NP -> N, NP -> Pronoun, NP -> AP NP, PrepP -> Prep NP, AP -> Adj, AP -> Adv Adj, V -> 'нравится', N -> 'цвет', Pronoun -> 'Мне', Adv -> 'очень']\n",
      "(S\n",
      "  (NP (Pronoun Мне))\n",
      "  (VP (Adv очень) (VP (V нравится) (NP (N цвет)))))\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Мне очень нравится цвет'\n",
    "tokens = tokenize(sentence)\n",
    "grammar = create_tree(sentence)\n",
    "print(grammar.productions())\n",
    "\n",
    "parser = nltk.ChartParser(grammar)\n",
    "trees = list(parser.parse(tokens))\n",
    "print(trees[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "08bda246",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[S -> NP VP,\n",
       " S -> NP VP C VP,\n",
       " VP -> V NP,\n",
       " VP -> VP NP,\n",
       " VP -> Adv VP,\n",
       " VP -> V NP PrepP,\n",
       " VP -> VP NP PrepP,\n",
       " VP -> V V,\n",
       " NP -> N,\n",
       " NP -> Pronoun,\n",
       " NP -> AP NP,\n",
       " PrepP -> Prep NP,\n",
       " AP -> Adj,\n",
       " AP -> Adv Adj,\n",
       " V -> 'нравится',\n",
       " N -> 'цвет',\n",
       " Pronoun -> 'Мне',\n",
       " Adv -> 'очень']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammar.productions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28a870a",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "99b80d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мне очень нравятся твои волосы\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Grammar does not cover some of the input words: \"'твои'\".",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20340/88749732.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChartParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrees\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mtrees\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\nltk\\parse\\chart.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, tokens, tree_class)\u001b[0m\n\u001b[0;32m   1472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1474\u001b[1;33m         \u001b[0mchart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchart_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1475\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtree_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1476\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\nltk\\parse\\chart.py\u001b[0m in \u001b[0;36mchart_parse\u001b[1;34m(self, tokens, trace)\u001b[0m\n\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1431\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1432\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_coverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1433\u001b[0m         \u001b[0mchart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_chart_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1434\u001b[0m         \u001b[0mgrammar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\nltk\\grammar.py\u001b[0m in \u001b[0;36mcheck_coverage\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    663\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    664\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\", \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{w!r}\"\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 665\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    666\u001b[0m                 \u001b[1;34m\"Grammar does not cover some of the \"\u001b[0m \u001b[1;34m\"input words: %r.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m             )\n",
      "\u001b[1;31mValueError\u001b[0m: Grammar does not cover some of the input words: \"'твои'\"."
     ]
    }
   ],
   "source": [
    "sentence = input()\n",
    "tokens = tokenize(sentence)\n",
    "tree = create_tree(sentence)\n",
    "parser = nltk.ChartParser(tree)\n",
    "trees = list(parser.parse(tokens))\n",
    "trees[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "37c9ccb4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "я пишу письмо старому другу\n",
      "(S\n",
      "  (NP (Pronoun я))\n",
      "  (VP\n",
      "    (VP (V пишу) (NP (N письмо)))\n",
      "    (NP (AP (Adj старому)) (NP (N другу)))))\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
